{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "PROJ_DIR = os.path.realpath(os.path.dirname(os.path.abspath('')))\n",
    "sys.path.append(os.path.join(PROJ_DIR,'src'))\n",
    "import xai_faithfulness_experiments_lib_edits as fl\n",
    "\n",
    "DICT_PATH_TRAIN = os.path.join(PROJ_DIR, 'data', 'cmnist_train_dict.pickle')\n",
    "DICT_PATH_TEST = os.path.join(PROJ_DIR, 'data', 'cmnist_test_dict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'x', 'y', 's_box', 's_digit', 's_area'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device}')\n",
    "BATCH_SIZE_TRAIN = 256\n",
    "BATCH_SIZE_TEST = 256\n",
    "\n",
    "model = fl.load_pretrained_cmnist_resnet18_model(os.path.join(PROJ_DIR,'assets','models','cmnist-resnet18.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8207, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loader = fl.get_cmnist_test_loader(DICT_PATH_TEST, BATCH_SIZE_TRAIN)\n",
    "test_hits = 0\n",
    "num_elems = 0\n",
    "for x_batch, y_batch in test_loader:\n",
    "    x_batch =  x_batch.to(device)\n",
    "    y_batch =  y_batch.to(device)\n",
    "    test_preds = model.forward(x_batch)        \n",
    "    test_hits += (test_preds.argmax(dim=1) == y_batch).float().sum()\n",
    "    num_elems += y_batch.shape[0]\n",
    "print(test_hits / num_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 32)\n",
      "[0.08951833516498424]\n",
      "[0.20045002645718005]\n",
      "[0.19793383839411702]\n",
      "[0.21131972993324646]\n",
      "[0.20581481330728316]\n",
      "[0.1998496519863237]\n",
      "[0.21131972993324646]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eirasf/miniconda3/envs/xai-anna/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/eirasf/miniconda3/envs/xai-anna/lib/python3.11/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:64: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import captum_generator as cg\n",
    "from quantus import PointingGame, AttributionLocalisation\n",
    "test_set = fl.CMNISTDataset(dict_file_path=DICT_PATH_TEST)\n",
    "mask_name = 's_area'\n",
    "\n",
    "#from matplotlib\n",
    "\n",
    "for elem in test_set.data.values():\n",
    "    x = torch.tensor(elem['x']).to(device)\n",
    "    y = torch.tensor(elem['y']).to(device)\n",
    "    rankings = cg.generate_rankings(x, y, model)\n",
    "    #pg = PointingGame(disable_warnings=True)\n",
    "    al = AttributionLocalisation(disable_warnings=True)\n",
    "    x_batch = x.unsqueeze(0).detach().cpu().numpy()\n",
    "    y_batch = y.unsqueeze(0).detach().cpu().numpy()\n",
    "\n",
    "    s_batch = np.expand_dims(np.expand_dims(elem[mask_name][0,:,:], axis = 0), axis = 0) # Only the first channel has elements\n",
    "    print(s_batch.shape)\n",
    "    for r in rankings:\n",
    "        r = r.sum(axis=0, keepdims=True)\n",
    "        score = al(model, \\\n",
    "                    x_batch, \\\n",
    "                    y_batch, \\\n",
    "                    np.expand_dims(r, axis = 0), \\\n",
    "                    s_batch)\n",
    "        print(score)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai-anna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
